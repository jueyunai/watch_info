import type { VercelRequest, VercelResponse } from '@vercel/node';

// å…è®¸çš„æ¥æºåˆ—è¡¨
const ALLOWED_ORIGINS = [
  // ç”Ÿäº§ç¯å¢ƒ
  'https://watcha.jueyunai.com',
  'https://watch-info.vercel.app',
  // å¼€å‘ç¯å¢ƒ
  'http://localhost:5173',
  'http://localhost:3000',
  'http://127.0.0.1:5173',
];

// éªŒè¯è¯·æ±‚æ¥æº
function isRequestAllowed(req: VercelRequest): { allowed: boolean; reason?: string } {
  const origin = req.headers.origin || '';
  const referer = req.headers.referer || '';

  // æ£€æŸ¥ Origin
  const isOriginAllowed = ALLOWED_ORIGINS.includes(origin);

  // æ£€æŸ¥ Refererï¼ˆæŸäº›åœºæ™¯ä¸‹ Origin å¯èƒ½ä¸ºç©ºï¼‰
  const isRefererAllowed = ALLOWED_ORIGINS.some(o => referer.startsWith(o));

  if (isOriginAllowed || isRefererAllowed) {
    return { allowed: true };
  }

  return {
    allowed: false,
    reason: `origin=${origin}, referer=${referer}`
  };
}

// æ”¯æŒçš„ LLM å‚å•†
type LLMProvider = 'minimax' | 'zhipu' | 'deepseek' | 'qwen' | 'openai';

interface LLMConfig {
  apiKey: string;
  baseUrl: string;
  model: string;
  maxTokens?: number;
}

// ä»ç¯å¢ƒå˜é‡è·å–å‚å•†é…ç½®
function getProviderConfig(provider: LLMProvider): LLMConfig | null {
  const configs: Record<LLMProvider, LLMConfig> = {
    minimax: {
      apiKey: process.env.VITE_MINIMAX_API_KEY || '',
      baseUrl: process.env.VITE_MINIMAX_BASE_URL || 'https://api.minimax.chat/v1',
      model: process.env.VITE_MINIMAX_MODEL || 'MiniMax-M2.1',
    },
    zhipu: {
      apiKey: process.env.VITE_ZHIPU_API_KEY || '',
      baseUrl: process.env.VITE_ZHIPU_BASE_URL || 'https://open.bigmodel.cn/api/paas/v4',
      model: process.env.VITE_ZHIPU_MODEL || 'glm-4-flash',
    },
    deepseek: {
      apiKey: process.env.VITE_DEEPSEEK_API_KEY || '',
      baseUrl: process.env.VITE_DEEPSEEK_BASE_URL || 'https://api.deepseek.com/v1',
      model: process.env.VITE_DEEPSEEK_MODEL || 'deepseek-chat',
    },
    qwen: {
      apiKey: process.env.VITE_QWEN_API_KEY || '',
      baseUrl: process.env.VITE_QWEN_BASE_URL || 'https://dashscope.aliyuncs.com/compatible-mode/v1',
      model: process.env.VITE_QWEN_MODEL || 'qwen-plus',
    },
    openai: {
      apiKey: process.env.VITE_OPENAI_API_KEY || '',
      baseUrl: process.env.VITE_OPENAI_BASE_URL || 'https://api.openai.com/v1',
      model: process.env.VITE_OPENAI_MODEL || 'gpt-4o',
    },
  };

  const config = configs[provider];
  return config?.apiKey ? config : null;
}

// è·å–ä¼˜å…ˆçº§åˆ—è¡¨
function getProviderPriority(): LLMProvider[] {
  const providers = process.env.VITE_LLM_PROVIDERS || 'minimax,zhipu,deepseek';
  return providers.split(',').map(p => p.trim()) as LLMProvider[];
}

function buildUrl(baseUrl: string): string {
  return baseUrl.includes('/chat/completions') ? baseUrl : `${baseUrl}/chat/completions`;
}

// è°ƒç”¨å•ä¸ªå‚å•†
async function callProvider(config: LLMConfig, messages: any[], stream: boolean, timeoutMs: number = 5000) {
  // ä½¿ç”¨ AbortController è®¾ç½®è¶…æ—¶ï¼ˆæµå¼å“åº”åº”è¯¥å¾ˆå¿«å¼€å§‹è¿”å›æ•°æ®ï¼‰
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), timeoutMs);

  try {
    const response = await fetch(buildUrl(config.baseUrl), {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.apiKey}`,
      },
      body: JSON.stringify({
        model: config.model,
        messages,
        max_tokens: config.maxTokens || 4000, // å¢åŠ  token æ•°é‡ï¼Œæ”¯æŒé•¿æŠ¥å‘Š
        temperature: 0.7,
        stream,
      }),
      signal: controller.signal,
    });

    clearTimeout(timeoutId);

    if (!response.ok) {
      const error = await response.text();
      throw new Error(`${config.model} è°ƒç”¨å¤±è´¥ (${response.status}): ${error}`);
    }

    return response;
  } catch (error) {
    clearTimeout(timeoutId);
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error(`${config.model} è¯·æ±‚è¶…æ—¶`);
    }
    throw error;
  }
}

export default async function handler(req: VercelRequest, res: VercelResponse) {
  const origin = req.headers.origin || '';

  // ğŸ”’ åŠ¨æ€è®¾ç½® CORSï¼ˆåªå¯¹å…è®¸çš„æ¥æºè®¾ç½®ï¼‰
  res.setHeader('Access-Control-Allow-Credentials', 'true');
  if (ALLOWED_ORIGINS.includes(origin)) {
    res.setHeader('Access-Control-Allow-Origin', origin);
  }
  res.setHeader('Access-Control-Allow-Methods', 'POST,OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');

  // OPTIONS é¢„æ£€è¯·æ±‚æ”¾è¡Œ
  if (req.method === 'OPTIONS') {
    res.status(200).end();
    return;
  }

  // ğŸ”’ éªŒè¯è¯·æ±‚æ¥æºï¼ˆé OPTIONS è¯·æ±‚ï¼‰
  const { allowed, reason } = isRequestAllowed(req);
  if (!allowed) {
    console.warn(`[LLM] æ‹’ç»éæ³•è¯·æ±‚: ${reason}`);
    return res.status(403).json({
      error: 'Forbidden',
      message: 'è¯·ä»å®˜æ–¹é¡µé¢è®¿é—®'
    });
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { messages, stream = false, provider: requestedProvider, max_tokens } = req.body;

    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Invalid messages' });
    }

    // ç¡®å®šè¦å°è¯•çš„å‚å•†åˆ—è¡¨
    let providers: LLMProvider[];
    if (requestedProvider) {
      // å‰ç«¯æŒ‡å®šäº†å‚å•†ï¼Œåªç”¨è¿™ä¸€ä¸ª
      providers = [requestedProvider as LLMProvider];
    } else {
      // ä½¿ç”¨ä¼˜å…ˆçº§åˆ—è¡¨
      providers = getProviderPriority();
    }

    console.log(`[LLM] å‚å•†ä¼˜å…ˆçº§: ${providers.join(' â†’ ')}`);

    // è®°å½•è¯·æ±‚ä½“å¤§å°
    const promptLength = JSON.stringify(messages).length;
    console.log(`[LLM] ä»»åŠ¡å¼€å§‹, æ¶ˆæ¯é•¿åº¦: ${promptLength} chars`);

    // ä¾æ¬¡å°è¯•å„å‚å•†
    let lastError: Error | null = null;
    for (const provider of providers) {
      const config = getProviderConfig(provider);
      if (!config) {
        console.log(`[LLM] ${provider} æœªé…ç½®ï¼Œè·³è¿‡`);
        continue;
      }

      try {
        // æ™ºè°±é€šå¸¸é¦–å­—è¾ƒæ…¢ï¼Œç»™ 30sï¼Œå…¶ä»–ç»™ 10s
        const timeoutMs = provider === 'zhipu' ? 30000 : 10000;
        console.log(`[LLM] å°è¯•: ${provider} / ${config.model} (è¶…æ—¶: ${timeoutMs}ms)`);

        // åˆå¹¶é…ç½®
        const finalConfig = {
          ...config,
          maxTokens: max_tokens || config.maxTokens
        };

        const response = await callProvider(finalConfig, messages, stream, timeoutMs);

        // æµå¼å“åº”
        if (stream && response.body) {
          res.setHeader('Content-Type', 'text/event-stream');
          res.setHeader('Cache-Control', 'no-cache');
          res.setHeader('Connection', 'keep-alive');

          const reader = response.body.getReader();

          while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            res.write(value); // ğŸ‘ˆ ç›´æ¥å†™å…¥äºŒè¿›åˆ¶å—ï¼Œä¸è¿›è¡Œ decodeï¼Œç¡®ä¿ SSE æ ¼å¼ä¸è¢«ç ´å
          }
          res.end();
          return;
        }

        // éæµå¼å“åº”
        const data = await response.json();
        console.log(`[LLM] ${provider} è°ƒç”¨æˆåŠŸ`);
        return res.status(200).json({
          ...data,
          _provider: provider, // è¿”å›å®é™…ä½¿ç”¨çš„å‚å•†
        });
      } catch (error) {
        console.error(`[LLM] ${provider} å¤±è´¥:`, error);
        lastError = error as Error;
        // ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
      }
    }

    // æ‰€æœ‰å‚å•†éƒ½å¤±è´¥
    return res.status(500).json({
      error: 'æ‰€æœ‰ LLM å‚å•†è°ƒç”¨å¤±è´¥',
      details: lastError?.message,
    });
  } catch (error) {
    console.error('[LLM] ä»£ç†é”™è¯¯:', error);
    res.status(500).json({
      error: 'LLM proxy error',
      message: error instanceof Error ? error.message : 'Unknown error',
    });
  }
}
